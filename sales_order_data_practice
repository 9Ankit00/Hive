#Store raw data into hdfs location
hadoop fs -put /home/cloudera/Desktop/Hive-Class-main/sales_order_data.csv  /tmp/hive_data_class/

#To check data is present in hdfs location
hadoop fs -ls /tmp/hive_data_class
Found 2 items
-rw-r--r--   1 cloudera supergroup        655 2022-09-12 14:15 /tmp/hive_data_class/depart_data.csv
-rw-r--r--   1 cloudera supergroup     360233 2022-09-18 08:24 /tmp/hive_data_class/sales_order_data.csv

#Create a internal hive table "sales_order_csv" which will store csv data sales_order_csv .. make sure to skip header row while creating table
hive> use hive_class_b1;
OK
Time taken: 0.022 seconds
hive> create table sales_order_csv(
    > ORDERNUMBER int,
    > QUANTITYORDERED int,
    > PRICEEACH float,
    > ORDERLINENUMBER int,
    > SALES FLOAT,
    > STATUS string,
    > QTR_ID int,
    > MONTH_ID int,
    > YEAR_ID int,
    > PRODUCTLINE string,
    > MSRP int,
    > PRODUCTCODE string,
    > PHONE string,
    > CITY string,
    > STATE string,
    > POSTALCODE string,
    > COUNTRY string,
    > TERRITORY string,
    > CONTACTLASTNAME string,
    > CONTACTFIRSTNAME string,
    > DEALSIZE string
    > )
    > row format delimited
    > fields terminated by ','
    > TBLPROPERTIES("skip.header.line.count"="1");
OK
hive> describe sales_order_csv;
OK
ordernumber         	int                 	                    
quantityordered     	int                 	                    
priceeach           	float               	                    
orderlinenumber     	int                 	                    
sales               	float               	                    
status              	string              	                    
qtr_id              	int                 	                    
month_id            	int                 	                    
year_id             	int                 	                    
productline         	string              	                    
msrp                	int                 	                    
productcode         	string              	                    
phone               	string              	                    
city                	string              	                    
state               	string              	                    
postalcode          	string              	                    
country             	string              	                    
territory           	string              	                    
contactlastname     	string              	                    
contactfirstname    	string              	                    
dealsize            	string              	                    
Time taken: 0.163 seconds, Fetched: 21 row(s)

#Load data from hdfs path into "sales_order_csv" 
hive> load data inpath '/tmp/hive_data_class/' into table sales_order_csv;
Loading data to table hive_class_b1.sales_order_csv
Table hive_class_b1.sales_order_csv stats: [numFiles=1, numRows=0, totalSize=360233, rawDataSize=0]
OK
Time taken: 0.322 seconds

hive> select  * from sales_order_csv limit 2;
OK
10107	30	95.7	2	2871.0	Shipped	1	2	2003	Motorcycles	95	S10_1678	2125557818	NYC	NY	10022	USA	NA	Yu	Kwai	Small
10121	34	81.35	5	2765.9	Shipped	2	5	2003	Motorcycles	95	S10_1678	26.47.1555	Reims		51100	FranceEMEA	Henriot	Paul	Small
Time taken: 0.254 seconds, Fetched: 2 row(s)


Time taken: 0.19 seconds

#Create an internal hive table which will store data in ORC format "sales_order_orc"
hive> create table sales_order_orc
    > (
    > ORDERNUMBER int,
    > QUANTITYORDERED int,
    > PRICEEACH float,
    > ORDERLINENUMBER int,
    > SALES float,
    > STATUS string,
    > QTR_ID int,
    > MONTH_ID int,
    > PRODUCTLINE string,
    > MSRP int,
    > PRODUCTCODE string,
    > PHONE string,
    > CITY string,
    > STATE string,
    > POSTALCODE string,
    > TERRITORY string,
    > CONTACTLASTNAME string,
    > CONTACTFIRSTNAME string,
    > DEALSIZE string
    > )
    > partitioned by (country string,year_id int)
    > stored as orc
    > ;
OK
Time taken: 0.394 second
 


#Load data from "sales_order_csv" into "sales_order_orc"
hive> insert overwrite table sales_order_orc partition(country,year_id) 
    > select 
    > ORDERNUMBER ,
    > QUANTITYORDERED ,
    > PRICEEACH ,
    > ORDERLINENUMBER ,
    > SALES ,
    > STATUS ,
    > QTR_ID ,
    > MONTH_ID ,
    > PRODUCTLINE ,
    > MSRP ,
    > PRODUCTCODE ,
    > PHONE ,
    > CITY ,
    > STATE ,
    > POSTALCODE ,
    > TERRITORY ,
    > CONTACTLASTNAME ,
    > CONTACTFIRSTNAME ,
    > DEALSIZE ,
    > country,year_id
    > 
    > from sales_order_csv;
 
#a. Calculatye total sales per year
hive> set mapreduce.jobs.reduces=5;
hive> select year_id, sum(sales) as total_sales from sales_order_orc group by year_id;
Query ID = cloudera_20220922162222_4fc802b5-47c3-4947-ba21-00dcb03d1b9e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0015, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0015/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0015
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 16:22:52,449 Stage-1 map = 0%,  reduce = 0%
2022-09-22 16:22:58,831 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.29 sec
2022-09-22 16:23:05,132 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.85 sec
MapReduce Total cumulative CPU time: 3 seconds 850 msec
Ended Job = job_1663884288379_0015
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.85 sec   HDFS Read: 258754 HDFS Write: 70 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 850 msec
OK
2003	3516979.547241211
2004	4724162.593383789
2005	1791486.7086791992

#d. In which quarter sales was minimum
hive> set hive.cli.print.header=true;
hive> select a.QTR_ID,a.year_id from (select sum(sales) totalsales,QTR_ID,year_id from sales_order_orc group by QTR_ID,year_id order by totalsales limit 1) a;  
Query ID = cloudera_20220922164141_f3baec5f-b982-4f73-b19d-4548d7f87d7f
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0022, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0022/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0022
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 16:41:57,009 Stage-1 map = 0%,  reduce = 0%
2022-09-22 16:42:03,197 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.6 sec
2022-09-22 16:42:10,435 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.6 sec
MapReduce Total cumulative CPU time: 2 seconds 730 msec
Ended Job = job_1663884288379_0022
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0023, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0023/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0023
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-22 16:42:18,169 Stage-2 map = 0%,  reduce = 0%
2022-09-22 16:42:22,462 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.78 sec
2022-09-22 16:42:27,648 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.17 sec
MapReduce Total cumulative CPU time: 2 seconds 170 msec
Ended Job = job_1663884288379_0023
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.73 sec   HDFS Read: 260070 HDFS Write: 386 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.17 sec   HDFS Read: 5626 HDFS Write: 7 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 900 msec
OK
a.qtr_id	a.year_id
1	2003
Time taken: 41.948 seconds, Fetched: 1 row(s)

#e. In which country sales was maximum and in which country sales was minimum
#maximum
hive> select sum(sales) totalsales ,country from sales_order_csv group by country order by totalsales TOP;
FAILED: ParseException line 1:96 extraneous input 'TOP' expecting EOF near '<EOF>'
hive> select a.country from (select sum(sales) totalsales ,country from sales_order_csv group by country order by totalsales desc limit 1) a ;
Query ID = cloudera_20220922170909_f81701b6-0aa4-4bae-8d64-33d04ed1c6c6
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0024, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0024/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0024
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 17:09:12,359 Stage-1 map = 0%,  reduce = 0%
2022-09-22 17:09:17,552 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.24 sec
2022-09-22 17:09:24,788 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.48 sec
MapReduce Total cumulative CPU time: 2 seconds 480 msec
Ended Job = job_1663884288379_0024
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0025, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0025/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0025
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-22 17:09:33,420 Stage-2 map = 0%,  reduce = 0%
2022-09-22 17:09:38,786 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec
2022-09-22 17:09:45,008 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.45 sec
MapReduce Total cumulative CPU time: 2 seconds 450 msec
Ended Job = job_1663884288379_0025
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.48 sec   HDFS Read: 370093 HDFS Write: 716 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.45 sec   HDFS Read: 5643 HDFS Write: 4 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 930 msec
OK
a.country
USA
Time taken: 38.469 seconds, Fetched: 1 row(s)

#minimum
hive> select a.country from (select sum(sales) totalsales ,country from sales_order_csv group by country order by totalsales asc limit 1) a ;
Query ID = cloudera_20220922171717_8b57f45a-0c0e-48d0-a8e2-5cf57405ffa8
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0029, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0029/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0029
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 17:17:14,330 Stage-1 map = 0%,  reduce = 0%
2022-09-22 17:17:19,545 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.23 sec
2022-09-22 17:17:26,770 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.35 sec
MapReduce Total cumulative CPU time: 2 seconds 350 msec
Ended Job = job_1663884288379_0029
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0030, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0030/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0030
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-22 17:17:33,351 Stage-2 map = 0%,  reduce = 0%
2022-09-22 17:17:38,585 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec
2022-09-22 17:17:44,784 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.1 sec
MapReduce Total cumulative CPU time: 2 seconds 100 msec
Ended Job = job_1663884288379_0030
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.35 sec   HDFS Read: 370200 HDFS Write: 716 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.1 sec   HDFS Read: 5652 HDFS Write: 8 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 450 msec
OK
a.country
Ireland
Time taken: 36.087 seconds, Fetched: 1 row(s)

