##Store raw data into hdfs location
hadoop fs -put /home/cloudera/Desktop/Hive-Class-main/sales_order_data.csv  /tmp/hive_data_class/

##To check data is present in hdfs location
hadoop fs -ls /tmp/hive_data_class
Found 2 items
-rw-r--r--   1 cloudera supergroup        655 2022-09-12 14:15 /tmp/hive_data_class/depart_data.csv
-rw-r--r--   1 cloudera supergroup     360233 2022-09-18 08:24 /tmp/hive_data_class/sales_order_data.csv

##Create a internal hive table "sales_order_csv" which will store csv data sales_order_csv .. make sure to skip header row while creating table
hive> use hive_class_b1;
OK
Time taken: 0.022 seconds
hive> create table sales_order_csv(
    > ORDERNUMBER int,
    > QUANTITYORDERED int,
    > PRICEEACH float,
    > ORDERLINENUMBER int,
    > SALES FLOAT,
    > STATUS string,
    > QTR_ID int,
    > MONTH_ID int,
    > YEAR_ID int,
    > PRODUCTLINE string,
    > MSRP int,
    > PRODUCTCODE string,
    > PHONE string,
    > CITY string,
    > STATE string,
    > POSTALCODE string,
    > COUNTRY string,
    > TERRITORY string,
    > CONTACTLASTNAME string,
    > CONTACTFIRSTNAME string,
    > DEALSIZE string
    > )
    > row format delimited
    > fields terminated by ','
    > TBLPROPERTIES("skip.header.line.count"="1");
OK
hive> describe sales_order_csv;
OK
ordernumber         	int                 	                    
quantityordered     	int                 	                    
priceeach           	float               	                    
orderlinenumber     	int                 	                    
sales               	float               	                    
status              	string              	                    
qtr_id              	int                 	                    
month_id            	int                 	                    
year_id             	int                 	                    
productline         	string              	                    
msrp                	int                 	                    
productcode         	string              	                    
phone               	string              	                    
city                	string              	                    
state               	string              	                    
postalcode          	string              	                    
country             	string              	                    
territory           	string              	                    
contactlastname     	string              	                    
contactfirstname    	string              	                    
dealsize            	string              	                    
Time taken: 0.163 seconds, Fetched: 21 row(s)

##Load data from hdfs path into "sales_order_csv" 
hive> load data inpath '/tmp/hive_data_class/' into table sales_order_csv;

Loading data to table hive_class_b1.sales_order_csv
Table hive_class_b1.sales_order_csv stats: [numFiles=1, numRows=0, totalSize=360233, rawDataSize=0]
OK
Time taken: 0.322 seconds

hive> select  * from sales_order_csv limit 2;

OK
10107	30	95.7	2	2871.0	Shipped	1	2	2003	Motorcycles	95	S10_1678	2125557818	NYC	NY	10022	USA	NA	Yu	Kwai	Small
10121	34	81.35	5	2765.9	Shipped	2	5	2003	Motorcycles	95	S10_1678	26.47.1555	Reims		51100	FranceEMEA	Henriot	Paul	Small
Time taken: 0.254 seconds, Fetched: 2 row(s)


Time taken: 0.19 seconds

#Create an internal hive table which will store data in ORC format "sales_order_orc"
hive> create table sales_order_orc
    > (
    > ORDERNUMBER int,
    > QUANTITYORDERED int,
    > PRICEEACH float,
    > ORDERLINENUMBER int,
    > SALES float,
    > STATUS string,
    > QTR_ID int,
    > MONTH_ID int,
    > PRODUCTLINE string,
    > MSRP int,
    > PRODUCTCODE string,
    > PHONE string,
    > CITY string,
    > STATE string,
    > POSTALCODE string,
    > TERRITORY string,
    > CONTACTLASTNAME string,
    > CONTACTFIRSTNAME string,
    > DEALSIZE string
    > )
    > partitioned by (country string,year_id int)
    > stored as orc
    > ;
OK
Time taken: 0.394 second
 


##Load data from "sales_order_csv" into "sales_order_orc"
hive> insert overwrite table sales_order_orc partition(country,year_id) 
    > select 
    > ORDERNUMBER ,
    > QUANTITYORDERED ,
    > PRICEEACH ,
    > ORDERLINENUMBER ,
    > SALES ,
    > STATUS ,
    > QTR_ID ,
    > MONTH_ID ,
    > PRODUCTLINE ,
    > MSRP ,
    > PRODUCTCODE ,
    > PHONE ,
    > CITY ,
    > STATE ,
    > POSTALCODE ,
    > TERRITORY ,
    > CONTACTLASTNAME ,
    > CONTACTFIRSTNAME ,
    > DEALSIZE ,
    > country,year_id
    > 
    > from sales_order_csv;
 
##a. Calculatte total sales per year
hive> set mapreduce.jobs.reduces=5;
hive> select year_id, sum(sales) as total_sales from sales_order_orc group by year_id;

Query ID = cloudera_20220922162222_4fc802b5-47c3-4947-ba21-00dcb03d1b9e
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0015, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0015/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0015
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 16:22:52,449 Stage-1 map = 0%,  reduce = 0%
2022-09-22 16:22:58,831 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.29 sec
2022-09-22 16:23:05,132 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.85 sec
MapReduce Total cumulative CPU time: 3 seconds 850 msec
Ended Job = job_1663884288379_0015
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.85 sec   HDFS Read: 258754 HDFS Write: 70 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 850 msec
OK
2003	3516979.547241211
2004	4724162.593383789
2005	1791486.7086791992

##b. Find a product for which maximum orders were placed
hive>select a.PRODUCTCODE from (select sum(QUANTITYORDERED) totalorder,PRODUCTCODE from sales_order_orc group by productcode order by totalorder desc limit 1) a;
Query ID = cloudera_20220923123434_2fa40687-dcb2-40b3-b623-89a95dd7e7c7
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663958987276_0011, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663958987276_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663958987276_0011
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-23 12:34:25,120 Stage-1 map = 0%,  reduce = 0%
2022-09-23 12:34:35,969 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.02 sec
2022-09-23 12:34:42,203 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.24 sec
MapReduce Total cumulative CPU time: 3 seconds 240 msec
Ended Job = job_1663958987276_0011
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663958987276_0012, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663958987276_0012/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663958987276_0012
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-23 12:34:49,311 Stage-2 map = 0%,  reduce = 0%
2022-09-23 12:34:54,497 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2022-09-23 12:35:00,735 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.2 sec
MapReduce Total cumulative CPU time: 2 seconds 200 msec
Ended Job = job_1663958987276_0012
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.24 sec   HDFS Read: 258079 HDFS Write: 3289 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.2 sec   HDFS Read: 8229 HDFS Write: 9 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 440 msec
OK
S18_3232
Time taken: 52.414 seconds, Fetched: 1 row(s)

##c. Calculate the total sales for each quarter
hive>  select sum(SALES) TOTALSALES,QTR_ID,year_id from sales_order_csv group by qtr_id,year_id;
Query ID = cloudera_20220923123939_ef841863-4ac5-4750-9ad8-e8aed47c3831
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663958987276_0013, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663958987276_0013/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663958987276_0013
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-23 12:39:50,974 Stage-1 map = 0%,  reduce = 0%
2022-09-23 12:39:56,228 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.26 sec
2022-09-23 12:40:04,572 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.5 sec
MapReduce Total cumulative CPU time: 2 seconds 500 msec
Ended Job = job_1663958987276_0013
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.5 sec   HDFS Read: 371437 HDFS Write: 253 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 500 msec
OK
445094.6897583008	1	2003
833730.6786499023	1	2004
1071992.3580932617	1	2005
562365.2218017578	2	2003
766260.7305297852	2	2004
719494.3505859375	2	2005
649514.5415039062	3	2003
1109396.2674560547	3	2004
1860005.094177246	4	2003
2014774.9167480469	4	2004


##d. In which quarter sales was minimum
hive> set hive.cli.print.header=true;
hive> select a.QTR_ID,a.year_id from (select sum(sales) totalsales,QTR_ID,year_id from sales_order_orc group by QTR_ID,year_id order by totalsales limit 1) a;  
Query ID = cloudera_20220922164141_f3baec5f-b982-4f73-b19d-4548d7f87d7f
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0022, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0022/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0022
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 16:41:57,009 Stage-1 map = 0%,  reduce = 0%
2022-09-22 16:42:03,197 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.6 sec
2022-09-22 16:42:10,435 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.6 sec
MapReduce Total cumulative CPU time: 2 seconds 730 msec
Ended Job = job_1663884288379_0022
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0023, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0023/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0023
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-22 16:42:18,169 Stage-2 map = 0%,  reduce = 0%
2022-09-22 16:42:22,462 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.78 sec
2022-09-22 16:42:27,648 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.17 sec
MapReduce Total cumulative CPU time: 2 seconds 170 msec
Ended Job = job_1663884288379_0023
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.73 sec   HDFS Read: 260070 HDFS Write: 386 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.17 sec   HDFS Read: 5626 HDFS Write: 7 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 900 msec
OK
a.qtr_id	a.year_id
1	2003
Time taken: 41.948 seconds, Fetched: 1 row(s)

##e. In which country sales was maximum and in which country sales was minimum
#maximum
hive> select sum(sales) totalsales ,country from sales_order_csv group by country order by totalsales TOP;
FAILED: ParseException line 1:96 extraneous input 'TOP' expecting EOF near '<EOF>'
hive> select a.country from (select sum(sales) totalsales ,country from sales_order_csv group by country order by totalsales desc limit 1) a ;
Query ID = cloudera_20220922170909_f81701b6-0aa4-4bae-8d64-33d04ed1c6c6
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0024, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0024/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0024
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 17:09:12,359 Stage-1 map = 0%,  reduce = 0%
2022-09-22 17:09:17,552 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.24 sec
2022-09-22 17:09:24,788 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.48 sec
MapReduce Total cumulative CPU time: 2 seconds 480 msec
Ended Job = job_1663884288379_0024
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0025, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0025/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0025
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-22 17:09:33,420 Stage-2 map = 0%,  reduce = 0%
2022-09-22 17:09:38,786 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec
2022-09-22 17:09:45,008 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.45 sec
MapReduce Total cumulative CPU time: 2 seconds 450 msec
Ended Job = job_1663884288379_0025
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.48 sec   HDFS Read: 370093 HDFS Write: 716 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.45 sec   HDFS Read: 5643 HDFS Write: 4 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 930 msec
OK
a.country
USA
Time taken: 38.469 seconds, Fetched: 1 row(s)

#minimum
hive> select a.country from (select sum(sales) totalsales ,country from sales_order_csv group by country order by totalsales asc limit 1) a ;
Query ID = cloudera_20220922171717_8b57f45a-0c0e-48d0-a8e2-5cf57405ffa8
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0029, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0029/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0029
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 17:17:14,330 Stage-1 map = 0%,  reduce = 0%
2022-09-22 17:17:19,545 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.23 sec
2022-09-22 17:17:26,770 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.35 sec
MapReduce Total cumulative CPU time: 2 seconds 350 msec
Ended Job = job_1663884288379_0029
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0030, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0030/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0030
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-22 17:17:33,351 Stage-2 map = 0%,  reduce = 0%
2022-09-22 17:17:38,585 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.87 sec
2022-09-22 17:17:44,784 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.1 sec
MapReduce Total cumulative CPU time: 2 seconds 100 msec
Ended Job = job_1663884288379_0030
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.35 sec   HDFS Read: 370200 HDFS Write: 716 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.1 sec   HDFS Read: 5652 HDFS Write: 8 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 450 msec
OK
a.country
Ireland
Time taken: 36.087 seconds, Fetched: 1 row(s)

##f. Calculate quartelry sales for each city
hive> select sum(sales) totalsales ,QTR_ID,city,year_id from sales_order_csv group by QTR_ID,city,year_id  order by year_id,city,qtr_id;
Query ID = cloudera_20220922173535_3ae1eb19-c829-41f8-a71c-5f6d6774139a
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0038, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0038/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0038
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-22 17:35:21,202 Stage-1 map = 0%,  reduce = 0%
2022-09-22 17:35:26,366 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.33 sec
2022-09-22 17:35:33,612 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.48 sec
MapReduce Total cumulative CPU time: 2 seconds 480 msec
Ended Job = job_1663884288379_0038
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663884288379_0039, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663884288379_0039/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663884288379_0039
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-22 17:35:43,129 Stage-2 map = 0%,  reduce = 0%
2022-09-22 17:35:47,348 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec
2022-09-22 17:35:54,589 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.2 sec
MapReduce Total cumulative CPU time: 2 seconds 200 msec
Ended Job = job_1663884288379_0039
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.48 sec   HDFS Read: 370911 HDFS Write: 9071 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.2 sec   HDFS Read: 14722 HDFS Write: 7964 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 680 msec
OK
totalsales	qtr_id	city	year_id
40321.60998535156	4	Aaarhus	2003
4219.2001953125	2	Barcelona	2003
44009.30993652344	4	Barcelona	2003
56181.320068359375	1	Bergamo	2003
40077.71026611328	4	Bergamo	2003
95277.17993164062	4	Bergen	2003
48710.92053222656	4	Boras	2003
63730.7802734375	4	Boston	2003
7277.35009765625	2	Brickhaven	2003
34992.39978027344	3	Brickhaven	2003
26115.800537109375	4	Bridgewater	2003
34100.030029296875	3	Brisbane	2003
8234.559936523438	4	Burbank	2003
42031.83020019531	3	Burlingame	2003
62305.47009277344	4	Burlingame	2003
1711.260009765625	2	Charleroi	2003
1637.199951171875	3	Charleroi	2003
28397.259521484375	3	Chatswood	2003
31569.430053710938	3	Espoo	2003
11432.33984375	1	Frankfurt	2003
27257.790283203125	4	Frankfurt	2003
37878.54992675781	4	Glen Waverly	2003
20350.949768066406	2	Glendale	2003
7600.1201171875	3	Glendale	2003
5142.14990234375	4	Glendale	2003
43488.740234375	4	Graz	2003
42083.499755859375	4	Helsinki	2003
58871.110107421875	1	Kobenhavn	2003
31363.1796875	4	Koln	2003
33847.61975097656	2	Las Vegas	2003
48874.28088378906	4	Lille	2003
26797.210083007812	4	Liverpool	2003
32376.29052734375	2	London	2003
70230.12927246094	4	London	2003
24159.14013671875	4	Los Angeles	2003
9748.999755859375	1	Lule	2003
41535.11022949219	4	Lyon	2003
44621.96008300781	1	Madrid	2003
100689.03051757812	2	Madrid	2003
47727.82019042969	3	Madrid	2003
112573.33026123047	4	Madrid	2003
55245.02014160156	1	Makati City	2003
22841.960083007812	4	Makati City	2003
51017.919860839844	1	Manchester	2003
52481.840087890625	2	Marseille	2003
60135.84033203125	2	Melbourne	2003
15947.290405273438	4	Montreal	2003
32647.809814453125	1	NYC	2003
93239.56018066406	2	NYC	2003
89600.84008789062	4	NYC	2003
16560.300048828125	2	Nantes	2003
12133.25	1	Nashua	2003
63981.449768066406	4	Nashua	2003
45738.38952636719	3	New Bedford	2003
64053.130615234375	4	New Bedford	2003
42498.760498046875	4	New Haven	2003
47191.76013183594	3	North Sydney	2003
41791.949462890625	4	North Sydney	2003
45078.759765625	4	Oslo	2003
37501.580322265625	3	Oulu	2003
38217.41046142578	2	Paris	2003
25624.880004882812	3	Paris	2003
55776.119873046875	3	Pasadena	2003
4512.47998046875	4	Pasadena	2003
27398.820434570312	1	Philadelphia	2003
40061.66003417969	4	Philadelphia	2003
44669.740478515625	4	Reggio Emilia	2003
18971.959716796875	2	Reims	2003
15146.31982421875	3	Reims	2003
38629.14001464844	2	Salzburg	2003
18695.579833984375	1	San Francisco	2003
121110.76025390625	4	San Francisco	2003
12398.56005859375	1	San Rafael	2003
122368.67102050781	3	San Rafael	2003
50360.88977050781	4	San Rafael	2003
51502.74108886719	4	Sevilla	2003
43657.47009277344	2	Singapore	2003
44219.36022949219	3	Singapore	2003
77809.37023925781	4	Singapore	2003
10640.290161132812	3	South Brisbane	2003
27098.800048828125	4	South Brisbane	2003
54701.999755859375	1	Stavern	2003
1474.6600341796875	4	Stavern	2003
17251.08056640625	3	Toulouse	2003
38098.240234375	4	Toulouse	2003
38662.209716796875	4	Vancouver	2003
38682.949462890625	4	White Plains	2003
60273.93981933594	4	Aaarhus	2004
71930.61041259766	3	Allentown	2004
44040.729736328125	4	Allentown	2004
30183.35009765625	4	Barcelona	2004
41696.68981933594	4	Bergamo	2004
16363.099975585938	3	Bergen	2004
53941.68981933594	3	Boras	2004
26677.350219726562	2	Boston	2004
15344.640014648438	3	Boston	2004
79982.13989257812	3	Brickhaven	2004
11528.52978515625	4	Brickhaven	2004
44130.52062988281	2	Bridgewater	2004
16118.479858398438	1	Brisbane	2004
18800.089721679688	1	Bruxelles	2004
47760.479736328125	3	Bruxelles	2004
37850.07958984375	1	Burbank	2004
2916.199951171875	4	Burlingame	2004
21782.699951171875	1	Cambridge	2004
14380.920043945312	2	Cambridge	2004
48828.71942138672	3	Cambridge	2004
54251.659912109375	4	Cambridge	2004
13463.480224609375	4	Charleroi	2004
41297.14050292969	3	Chatswood	2004
37905.14990234375	4	Chatswood	2004
26906.68017578125	1	Cowes	2004
51334.15966796875	4	Cowes	2004
38784.470458984375	1	Dublin	2004
18971.959838867188	3	Dublin	2004
31018.230102539062	2	Espoo	2004
37266.48937988281	1	Frankfurt	2004
9214.969970703125	4	Frankfurt	2004
50432.549560546875	1	Gensve	2004
67281.00903320312	3	Gensve	2004
12334.819580078125	3	Glen Waverly	2004
29343.349975585938	4	Glendale	2004
42744.0595703125	3	Helsinki	2004
36079.010009765625	2	Kobenhavn	2004
24078.610107421875	4	Kobenhavn	2004
68943.40051269531	4	Koln	2004
34453.84973144531	3	Las Vegas	2004
14449.609741210938	4	Las Vegas	2004
20178.1298828125	1	Lille	2004
50408.25	2	Liverpool	2004
8477.219970703125	1	London	2004
13739.900024414062	4	London	2004
23889.320068359375	1	Los Angeles	2004
66005.8798828125	4	Lule	2004
101339.13977050781	1	Lyon	2004
105491.33990478516	1	Madrid	2004
119656.04028320312	2	Madrid	2004
21986.269897460938	3	Madrid	2004
203007.4793701172	4	Madrid	2004
15928.750244140625	4	Makati City	2004
106789.88977050781	4	Manchester	2004
20136.859985351562	4	Marseille	2004
49637.57067871094	1	Melbourne	2004
91221.99914550781	4	Melbourne	2004
25928.750244140625	2	Minato-ku	2004
55888.65026855469	4	Minato-ku	2004
24564.530029296875	2	Montreal	2004
34993.92004394531	3	Munich	2004
71860.779296875	2	NYC	2004
63027.92004394531	3	NYC	2004
210410.85986328125	4	NYC	2004
61310.880126953125	3	Nantes	2004
23031.589599609375	4	Nantes	2004
55570.59973144531	4	Nashua	2004
49504.379150390625	4	New Bedford	2004
36973.309814453125	2	New Haven	2004
8722.1201171875	1	Newark	2004
27987.069580078125	2	Newark	2004
50490.64013671875	1	Osaka	2004
17114.43017578125	2	Osaka	2004
34145.47021484375	3	Oslo	2004
17813.40008544922	2	Oulu	2004
51172.649658203125	1	Paris	2004
27931.210083007812	2	Paris	2004
2173.60009765625	3	Paris	2004
89436.60034179688	4	Paris	2004
7287.240234375	2	Philadelphia	2004
76441.41040039062	4	Philadelphia	2004
56421.650390625	3	Reggio Emilia	2004
48895.59014892578	4	Reims	2004
6693.2802734375	3	Salzburg	2004
45001.10986328125	4	Salzburg	2004
87489.23010253906	1	San Diego	2004
30348.720336914062	4	San Francisco	2004
64600.33984375	2	San Jose	2004
48922.76989746094	1	San Rafael	2004
93928.72961425781	3	San Rafael	2004
113622.75903320312	4	San Rafael	2004
3220.8800659179688	4	Sevilla	2004
24219.58984375	1	Singapore	2004
45788.72009277344	2	Singapore	2004
46030.7197265625	3	Singapore	2004
60422.530029296875	4	Stavern	2004
44758.12951660156	2	Strasbourg	2004
94117.25988769531	3	Torino	2004
31302.500244140625	2	Tsawassen	2004
43332.349609375	3	Tsawassen	2004
36576.70983886719	4	Vancouver	2004
5759.419921875	1	Versailles	2004
59074.90026855469	4	Versailles	2004
46873.04016113281	4	White Plains	2004
6166.7998046875	2	Allentown	2005
31606.72021484375	1	Boras	2005
48316.89001464844	2	Boston	2005
31474.7802734375	1	Brickhaven	2005
31648.469970703125	2	Bridgewater	2005
8411.949829101562	2	Bruxelles	2005
13529.570190429688	1	Burlingame	2005
16628.16015625	1	Charleroi	2005
43971.429931640625	2	Chatswood	2005
51373.49072265625	1	Espoo	2005
14378.089965820312	2	Glen Waverly	2005
3987.199951171875	1	Glendale	2005
8775.159912109375	1	Graz	2005
26422.819458007812	1	Helsinki	2005
26012.87060546875	2	Kobenhavn	2005
40802.8095703125	2	Liverpool	2005
207555.18994140625	1	Madrid	2005
119242.98052978516	2	Madrid	2005
2317.43994140625	1	Marseille	2005
38191.38977050781	1	Minato-ku	2005
553.9500122070312	2	Minato-ku	2005
33692.97009277344	2	Montreal	2005
59617.39978027344	1	Nantes	2005
43784.69012451172	2	Nantes	2005
48578.95935058594	1	New Bedford	2005
46518.99951171875	2	Newark	2005
65012.41955566406	1	North Sydney	2005
49055.40026855469	1	Oulu	2005
20321.52978515625	1	Paris	2005
14066.7998046875	2	Paris	2005
44273.359436035156	1	Pasadena	2005
41509.94006347656	2	Reggio Emilia	2005
52029.07043457031	1	Reims	2005
59475.100036621094	2	Salzburg	2005
54203.6201171875	1	San Francisco	2005
95409.93041992188	2	San Jose	2005
205993.9287109375	1	San Rafael	2005
7261.75	2	San Rafael	2005
4175.60009765625	1	Singapore	2005
2587.5799560546875	2	Singapore	2005
21730.029907226562	1	South Brisbane	2005
35680.350341796875	2	Strasbourg	2005
15139.1201171875	1	Toulouse	2005
Time taken: 42.19 seconds, Fetched: 234 row(s)

##h. Find a month for each year in which maximum number of quantities were sold
hive> select DATE_FORMAT(add_months('0001-00-01',a.month_id),'MMMM'),a.year_id from (select sum(QUANTITYORDERED) QUANTITY,month_id,year_id,max(sum(QUANTITYORDERED)) over(partition by year_id) as max_quantity from sales_order_csv group by month_id,year_id) a where a.QUANTITY=a.max_quantity;
Query ID = cloudera_20220923121717_69fcc0b8-a71e-4578-b72f-f72d951cf6d7
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663958987276_0007, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663958987276_0007/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663958987276_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-09-23 12:18:03,801 Stage-1 map = 0%,  reduce = 0%
2022-09-23 12:18:09,089 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.16 sec
2022-09-23 12:18:17,444 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.36 sec
MapReduce Total cumulative CPU time: 2 seconds 360 msec
Ended Job = job_1663958987276_0007
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1663958987276_0008, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1663958987276_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1663958987276_0008
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-09-23 12:18:24,903 Stage-2 map = 0%,  reduce = 0%
2022-09-23 12:18:31,408 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.94 sec
2022-09-23 12:18:37,628 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.53 sec
MapReduce Total cumulative CPU time: 2 seconds 530 msec
Ended Job = job_1663958987276_0008
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.36 sec   HDFS Read: 370190 HDFS Write: 792 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.53 sec   HDFS Read: 8718 HDFS Write: 37 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 890 msec
OK
November	2003
November	2004
May	2005

